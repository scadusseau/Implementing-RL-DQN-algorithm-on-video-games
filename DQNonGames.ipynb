{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQNonGames.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scadusseau/Implementing-RL-DQN-algorithm-on-video-games/blob/master/DQNonGames.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-vectzxT3LR",
        "colab_type": "text"
      },
      "source": [
        "# Implementing a DQN algorithm on video games https://www.youtube.com/watch?v=5fHngyN8Qhw"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipM9nw9mUKSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3toMFyCQUWkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Bon ici on utilise une astuce pour pouvoir afficher l'image sur google colab\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg libav-tools xorg-dev libsdl2-dev swig cmake > /dev/null 2>&1\n",
        "!apt-get install x11-utils > /dev/null 2>&1\n",
        "\n",
        "!pip install gym[atari]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg1SQNpgUXl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo apt-get install xvfb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7otowTo3Ud-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt install swig cmake libopenmpi-dev zlib1g-dev\n",
        "!pip install stable-baselines==2.5.1 box2d box2d-kengz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CsOo2EKUiSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install atari-py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1KDiFAjUjvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import HTML\n",
        "\n",
        "from pyvirtualdisplay import Display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYKc8fcRUrMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Pour l'affichage\n",
        "display = Display(visible=0, size=(800, 600))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjbBaiIeUshN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "  def __init__(self, max_size, input_shape, n_actions, discrete=False):\n",
        "    self.mem_size = max_size\n",
        "    self.mem_cntr = 0\n",
        "    self.input_shape = input_shape\n",
        "    self.discrete = discrete\n",
        "    self.state_memory = np.zeros((self.mem_size, input_shape))\n",
        "    self.new_state_memory = np.zeros((self.mem_size, input_shape))\n",
        "    dtype = np.int8 if self.discrete else np.float32\n",
        "    self.action_memory = np.zeros((self.mem_size, n_actions), dtype = dtype)\n",
        "    self.reward_memory = np.zeros(self.mem_size)\n",
        "    self.terminal_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "\n",
        "  def store_transition(self, state, action, reward, state_, done):\n",
        "    index = self.mem_cntr % self.mem_size\n",
        "    self.state_memory[index] = state\n",
        "    self.new_state_memory[index] = state_\n",
        "    self.reward_memory[index] = reward\n",
        "    self.terminal_memory[index] = 1 - int(done)\n",
        "    if self.discrete:\n",
        "      actions = np.zeros(self.action_memory.shape[1])\n",
        "      actions[action] = 1.0\n",
        "      self.action_memory[index] = actions\n",
        "    else:\n",
        "      self.action_memory[index] = action\n",
        "    self.mem_cntr += 1\n",
        "\n",
        "  def sample_buffer(self, batch_size):\n",
        "    max_mem = min(self.mem_cntr, self.mem_size)\n",
        "    batch = np.random.choice(max_mem, batch_size)\n",
        "\n",
        "    states = self.state_memory[batch]\n",
        "    states_ = self.new_state_memory[batch]\n",
        "    rewards = self.reward_memory[batch]\n",
        "    actions = self.action_memory[batch]\n",
        "    terminal = self.terminal_memory[batch]\n",
        "\n",
        "    return states, actions, rewards, states_, terminal\n",
        "\n",
        "def build_dqn(lr, n_actions, input_dims, fc1_dims, fc2_dims):\n",
        "  model = Sequential([Dense(fc1_dims, input_shape=(input_dims, )),\n",
        "                      Activation('relu'),\n",
        "                      Dense(fc2_dims),\n",
        "                      Activation('relu'),\n",
        "                      Dense(n_actions)])\n",
        "  model.compile(optimizer=Adam(lr=lr), loss='mse')\n",
        "\n",
        "  return model\n",
        "\n",
        "class Agent(object):\n",
        "  def __init__(self, alpha, gamma, n_actions, epsilon, batch_size,\n",
        "               input_dims, epsilon_dec=0.996, epsilon_end=0.01,\n",
        "               mem_size=1000000, fname='dqn_model.h5'):\n",
        "    self.action_space = [i for i in range(n_actions)]\n",
        "    self.n_actions = n_actions\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "    self.epsilon_dec = epsilon_dec\n",
        "    self.epsilon_min = epsilon_end\n",
        "    self.batch_size = batch_size\n",
        "    self.model_file = fname\n",
        "\n",
        "    self.memory = ReplayBuffer(mem_size, input_dims, n_actions, discrete=True)\n",
        "    self.q_eval = build_dqn(alpha, n_actions, input_dims, 256, 256)\n",
        "  \n",
        "  def remember(self, state, action, reward, new_state, done):\n",
        "    self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "  def choose_action(self, state):\n",
        "    state = state[np.newaxis, :]\n",
        "    rand = np.random.random()\n",
        "    if rand < self.epsilon:\n",
        "      action = np.random.choice(self.action_space)\n",
        "    else:\n",
        "      actions = self.q_eval.predict(state)\n",
        "      action = np.argmax(actions)\n",
        "\n",
        "    return action\n",
        "\n",
        "  def learn(self):\n",
        "    if self.memory.mem_cntr < self.batch_size:\n",
        "      return\n",
        "    state, action, reward, new_state, done = self.memory.sample_buffer(self.batch_size)\n",
        "    action_values = np.array(self.action_space, dtype=np.int8)\n",
        "    action_indices = np.dot(action, action_values)\n",
        "\n",
        "    q_eval = self.q_eval.predict(state)\n",
        "    q_next = self.q_eval.predict(new_state)\n",
        "\n",
        "    q_target = q_eval.copy()\n",
        "\n",
        "    batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
        "\n",
        "    q_target[batch_index, action_indices] = reward + self.gamma*np.max(q_next, axis=1)*done\n",
        "\n",
        "    _ = self.q_eval.fit(state, q_target, verbose=0)\n",
        "\n",
        "    self.epsilon = self.epsilon*self.epsilon_dec if self.epsilon > self.epsilon_min else self.epsilon_min\n",
        "\n",
        "  def save_model(self):\n",
        "    path = \"/content/gdrive/My Drive/\"+self.model_file \n",
        "    self.q_eval.save(path)\n",
        "\n",
        "  def load_model(self):\n",
        "    path = \"/content/gdrive/My Drive/\"+self.model_file  \n",
        "    self.q_eval = load_model(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro7L5odmVCqY",
        "colab_type": "text"
      },
      "source": [
        "# **For Lunar Lander**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1xJ_gKpU1Eh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "\n",
        "env = wrap_env(gym.make('LunarLander-v2'))\n",
        "n_games = 500\n",
        "agent = Agent(gamma=0.99, epsilon=1.0, alpha=0.0005, input_dims=8, n_actions = 4,\n",
        "              mem_size=1000000, batch_size=64, epsilon_end=0.01)\n",
        "  \n",
        "scores = []\n",
        "eps_history = []\n",
        "\n",
        "for i in range(n_games):\n",
        "  done = False\n",
        "  score = 0\n",
        "  observation = env.reset()\n",
        "  while not done:\n",
        "    action = agent.choose_action(observation)\n",
        "    observation_, reward, done, info = env.step(action)\n",
        "    score += reward\n",
        "    agent.remember(observation, action, reward, observation_, done)\n",
        "    observation = observation_\n",
        "    agent.learn()\n",
        "\n",
        "  eps_history.append(agent.epsilon)\n",
        "  scores.append(score)\n",
        "\n",
        "  avg_score = np.mean(scores[max(0, i-100):(i+1)])\n",
        "  print('episode ', i, 'score %.2f' % score, 'average score %.2f' % avg_score)\n",
        "\n",
        "  if i % 10 == 0 and i > 0:\n",
        "    agent.save_model()\n",
        "\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2frw19jVIfv",
        "colab_type": "text"
      },
      "source": [
        "# **For Space Invader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqAynxPKU91D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import cv2\n",
        "\n",
        "def preprocess(img):\n",
        "    return np.matrix.flatten(cv2.resize(cv2.cvtColor(img, cv2.COLOR_RGB2GRAY), (84, 90)))\n",
        "\n",
        "env = wrap_env(gym.make('SpaceInvaders-v0'))\n",
        "n_games = 500\n",
        "agent = Agent(gamma=0.99, epsilon=1.0, alpha=0.0005, input_dims=7560, n_actions = env.action_space.n,\n",
        "              mem_size=1000000, batch_size=64, epsilon_end=0.01)\n",
        "  \n",
        "#agent.load_model()\n",
        "\n",
        "scores = []\n",
        "eps_history = []\n",
        "\n",
        "for i in range(n_games):\n",
        "  done = False\n",
        "  score = 0\n",
        "  observation = env.reset()\n",
        "  observation = preprocess(observation)\n",
        "  while not done:\n",
        "    action = agent.choose_action(observation)\n",
        "    observation_, reward, done, info = env.step(action)\n",
        "    observation_ = preprocess(observation_)\n",
        "    score += reward\n",
        "    agent.remember(observation, action, reward, observation_, done)\n",
        "    observation = observation_\n",
        "    agent.learn()\n",
        "\n",
        "  eps_history.append(agent.epsilon)\n",
        "  scores.append(score)\n",
        "\n",
        "  avg_score = np.mean(scores[max(0, i-100):(i+1)])\n",
        "  print('episode ', i, 'score %.2f' % score, 'average score %.2f' % avg_score)\n",
        "\n",
        "  if i % 10 == 0 and i > 0:\n",
        "    agent.save_model()\n",
        "\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}